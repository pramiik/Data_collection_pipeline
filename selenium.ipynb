{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scraper import scraper\n",
    "\n",
    "bot = scraper()\n",
    "bot.click_search_bar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bot.search()\n",
    "bot.click_search_button()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot.close_login_popup()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = bot.finding_containers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from selenium.webdriver import chrome\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "list_books = container.find_elements(By.XPATH, '//tr')#/td[2]/a[@class=\"bookTitle\"]')\n",
    "len(list_books)\n",
    "\n",
    "#get_attribute \n",
    "#/html/body/div[2]/div[3]/div[1]/div[2]/div[2]/table/tbody/tr[1]/td[2]/a\n",
    "# './' indicates the first children, so it'll look for the children in the direct children. \n",
    "#  if we use .// then it looks for the tag all over the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Find the container containing all of the links you want \n",
    " Print the length of the list of webelements \n",
    " If you get 20, then you should be getting what you want. \n",
    " For each of those webelements, use the get_attribute method to extract the href of each of those webelements. \n",
    "\n",
    "//tr/td[2]/a[@class=\"bookTitle\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It\\nby Stephen King (Goodreads Author)\\n4.25 avg rating — 937,764 ratings — published 1986 — 139 editions\\nWant to Read\\nRate this book\\n1 of 5 stars\\n2 of 5 stars\\n3 of 5 stars\\n4 of 5 stars\\n5 of 5 stars\\nGet a copy'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_books[1].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_urls = []\n",
    "\n",
    "for books in list_books:\n",
    "    list_urls.append(books.find_element(By.TAG_NAME, 'a').get_attribute('href'))\n",
    "    \n",
    "#list_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def get_images(self):\n",
    "        \n",
    "        This method will return a list of urls to the images\n",
    "        \n",
    "        elem = self.driver.find_element(By.XPATH, '//div[@class=\"row row-cols-1 row-cols-sm-1  row-cols-md-5 row-cols-xl-5  row-cols-xxl-5 \"]')\n",
    "        columns = elem.find_elements(By.XPATH, './div[@class=\"col p-0 gcol\"]')\n",
    "        self.src_list = []\n",
    "        for column in columns:\n",
    "            images = column.find_elements(By.XPATH, './/img[@class=\"item-image img-fluid\"]')\n",
    "            for img in images:\n",
    "                try:\n",
    "                    self.src_list.append(img.get_attribute('src'))\n",
    "                except:\n",
    "                    print('No source found')\n",
    "        return self.src_list\n",
    "\n",
    "get_images\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "\n",
    "for url in list_urls:\n",
    "    page = requests.get(url)\n",
    "\n",
    "    \n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    print(soup.prettify())\n",
    "    image_tags = soup.find_all('img', id='coverImage')\n",
    "    links = []\n",
    "    for image_tag in image_tags:\n",
    "        links.append(image_tag['src'])\n",
    "    \n",
    "   \n",
    "    urllib.request.urlretrieve(links[0], \"images/innovators.jpg\")\n",
    "\n",
    "links\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import uuid\n",
    "from uuid import UUID\n",
    "\n",
    "info_dict = {\n",
    "                'urls' :[],\n",
    "                'unique ID' :[],\n",
    "                'book title' :[],\n",
    "                'author name' :[],\n",
    "                'rating' :[],\n",
    "                'book description' :[],\n",
    "                'book_cover_links' :[],\n",
    "                'v4 UUID' :[]\n",
    "                }\n",
    "\n",
    "for urls in list_urls[0:20]:\n",
    "\n",
    "    bot.driver.get(urls)\n",
    "    info_dict['urls'].append(urls)\n",
    "\n",
    "\n",
    "    ur = re.findall('\\d+', urls)\n",
    "    info_dict['unique ID'].append(ur[0])\n",
    "    \n",
    "\n",
    "    book_title = bot.driver.find_element(By.XPATH, '//*[@id=\"bookTitle\"]')\n",
    "    info_dict['book title'].append(book_title.text)\n",
    "\n",
    "    author_name = bot.driver.find_element(By.XPATH, '//*[@id=\"bookAuthors\"]')\n",
    "    info_dict['author name'].append(author_name.text)\n",
    "\n",
    "    rating = bot.driver.find_element(By.XPATH, '//*[@id=\"bookMeta\"]/span[2]')\n",
    "    info_dict['rating'].append(rating.text)\n",
    "\n",
    "    book_description = bot.driver.find_element(By.XPATH, '//*[@id=\"descriptionContainer\"]')\n",
    "    info_dict['book description'].append(book_description.text)\n",
    "\n",
    "\n",
    "\n",
    "    bot.driver.get(urls)\n",
    "    image = bot.driver.find_element(By.XPATH, '//*[@id=\"imagecol\"]/div[1]/div[1]/a')\n",
    "    #print(f'image is {image}')\n",
    "    imag_dic = image.find_element(By.TAG_NAME, 'img').get_attribute('src')\n",
    "    #print( f'imag dic {imag_dic}')\n",
    "    #print(image)\n",
    "    #img= image.get_attribute('src')\n",
    "    info_dict['book_cover_links'].append(imag_dic)\n",
    "\n",
    "\n",
    "\n",
    "    uu_id = uuid.uuid4()\n",
    "    uuid_str = str(uu_id)\n",
    "    info_dict['v4 UUID'].append(uuid_str)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for urls in list_urls[0:]:\n",
    "    bot.driver.get(urls)\n",
    "    image = bot.driver.find_element(By.XPATH, '//*[@id=\"imagecol\"]/div[1]/div[1]/a')\n",
    "    #print(f'image is {image}')\n",
    "    imag_dic = image.find_element(By.TAG_NAME, 'img').get_attribute('src')\n",
    "    #print( f'imag dic {imag_dic}')\n",
    "\n",
    "    #print(image)\n",
    "    #img= image.get_attribute('src')\n",
    "    info_dict['book cover links'].append(imag_dic)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "folder = r'raw_data'\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "\n",
    "with open(\"/home/pramika/Documents/Aicore/data_collection_project/raw_data/data.json\", \"w+\") as f:\n",
    "    json.dump(info_dict, f)\n",
    "\n",
    "#with open('data.json', mode='w') as f:\n",
    " #   json.dump(info_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "json file method\n",
    "\n",
    "\n",
    "test_dict = {'a': 3, 'b': 4}\n",
    "new_json = json.dumps(test_dict)\n",
    "print(new_json)\n",
    "print(type(new_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import json\n",
    "\n",
    "\n",
    "class UUIDEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, UUID):\n",
    "            # if the obj is uuid, we simply return the value of uuid\n",
    "            return obj.hex\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "page = get.request()\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "print(soup.prettify())\n",
    "image_tags = soup.find_all('img', class_='t0fcAb')\n",
    "links = []\n",
    "for image_tag in image_tags:\n",
    "    links.append(image_tag['src'])\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "for book_cover_links in info_dict:\n",
    "    bot.driver.get(urls)\n",
    "    with open('.jpg','wb') as f:\n",
    "                                pict = requests.get(imag_dic)\n",
    "                                f.write(pict.content)\n",
    "                                f.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "def download_images(src: str, animal: str, i: int) -> None:\n",
    "    urllib.request.urlretrieve(src, f\"{animal}_{i}.jpg\")\n",
    "\n",
    "\n",
    "for link in self.lego_links[0:]:\n",
    "                self.driver.get(link)\n",
    "                try:\n",
    "                    time.sleep(1)\n",
    "                    Product_name = self.driver.find_element(By.XPATH,'//h1[@data-test=\"product-overview-name\"]')\n",
    "                    name = (Product_name.text).replace(' ','_').replace(',','').replace('-','')\n",
    "                    img_container = self.driver.find_element(By.XPATH,'//picture[@class = \"Picturestyles__Container-j8hf1d-0 bVuOVw LazyImagestyles__Picture-sc-1gcjd00-1 ddKWmr\"]')\n",
    "                    find_image = img_container.find_element(By.TAG_NAME,'img').get_attribute('src')\n",
    "                    self.Image_dict['Lego_images'].append(find_image)\n",
    "                        #os.mkdir(os.path.join(os.getcwd(),name))\n",
    "                        #os.chdir(os.path.join(os.getcwd(),name))\n",
    "                        #name = re.sub('\\s+','_',(Product_name.text))\n",
    "                    with open(name + '.jpg','wb') as f:\n",
    "                            pict = requests.get(find_image)\n",
    "                            f.write(pict.content)\n",
    "                            f.close"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4f77f66f7e6696f1ddc190b944d805f09f35bc5ddda9b42e9ab4ac8a03c889de"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('selenium')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
